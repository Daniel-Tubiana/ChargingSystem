{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": [],
   "gpuType": "T4"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "cells": [
  {
   "cell_type": "code",
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv, VecNormalize\n",
    "from stable_baselines3.common.env_util import make_vec_env\n",
    "\n",
    "from stable_baselines3 import PPO,A2C\n",
    "import time"
   ],
   "metadata": {
    "id": "IO8xbn6PhGPH"
   },
   "execution_count": 1,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "# Choose Environment rewards type\n",
    "#ENV_type = 0 -> Continues\n",
    "#ENV_type = 1 -> Discrete\n",
    "ENV_type =  0\n",
    "\n",
    "# Set name of run\n",
    "# TODO: change test name\n",
    "#run_name = 'Continues_reward_4_V5_PPO_'\n",
    "run_name =  'SOC_reward_during_episode'"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# Load data\n",
    "#path = 'G:/My Drive/לימודים/פרוייקט גמר/Data/sim data' # small dataset\n",
    "\n",
    "path = 'C:/Users/dtubiana/PycharmProjects/chargeSystem/simulated_data'\n",
    "\n",
    "os.chdir(path)\n",
    "os.listdir(path)\n",
    "\n",
    "df_evs_train =pd.read_excel('usr_ev_data_train.xlsx')\n",
    "df_evs_test =pd.read_excel('usr_ev_data_test.xlsx')\n",
    "\n",
    "#df_evs_train =pd.read_excel('usr_ev_data_train_max_2030.xlsx')\n",
    "#df_evs_test =pd.read_excel('usr_ev_data_test_max_2030.xlsx')\n",
    "\n",
    "df_build = pd.read_excel('Building_data.xlsx')"
   ],
   "metadata": {
    "id": "psnQR74nhOqJ"
   },
   "execution_count": 3,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "df_evs_train = df_evs_train.drop(columns=['Unnamed: 0'])\n",
    "df_build = df_build.drop(columns=['Unnamed: 0'])\n",
    "df_evs_test = df_evs_test.drop(columns=['Unnamed: 0'])\n",
    "# create data frame to output with Initial soc"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "df_evs_out = pd.DataFrame()\n",
    "df_evs_out['initial_SOC']  = pd.DataFrame(df_evs_test)['SOC'] # get initial soc to calc soc diffrance"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "#df_evs_train\n",
    "#df_evs_test"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\dtubiana\\PycharmProjects\\chargeSystem\\venv\\lib\\site-packages\\gymnasium\\utils\\passive_env_checker.py:42: UserWarning: \u001B[33mWARN: A Box observation space has an unconventional shape (neither an image, nor a 1D vector). We recommend flattening the observation to have only a 1D vector or use a custom policy to properly process the data. Actual observation shape: (189, 5)\u001B[0m\n",
      "  logger.warn(\n"
     ]
    }
   ],
   "source": [
    "# change directory to import EVs environment and create multi environment\n",
    "os.chdir('C:/Users/dtubiana/PycharmProjects/chargeSystem')\n",
    "\n",
    "df_evs_to_load = df_evs_train.to_dict('records')\n",
    "df_build_surplus_pwr = df_build['surplus_power[kw]'].to_numpy()\n",
    "\n",
    "N_environments = 16\n",
    "if ENV_type == 0:\n",
    "    \"Continues environment\"\n",
    "    from gymnasium.envs.registration import register\n",
    "\n",
    "    register(\n",
    "        id= 'EVChargingEnv-v0',\n",
    "        entry_point='EVChargingEnv_continues_reward:make_ev_charging_env',\n",
    "        kwargs={'evs': df_evs_to_load, 'power_limit': df_build_surplus_pwr})\n",
    "\n",
    "\n",
    "    env = make_vec_env('EVChargingEnv-v0', n_envs=N_environments, seed=42)\n",
    "\n",
    "    env = VecNormalize(env, norm_obs=True, norm_reward=True, norm_obs_keys=['evs','power_allowed'])\n",
    "\n",
    "elif ENV_type == 1:\n",
    "    \"Discrete environment\"\n",
    "    from gymnasium.envs.registration import register\n",
    "\n",
    "    register(\n",
    "        id= 'EVChargingEnv-v0',\n",
    "        entry_point='EVChargingEnv_discrete_reward:make_ev_charging_env',\n",
    "        kwargs={'evs': df_evs_to_load, 'power_limit': df_build_surplus_pwr})\n",
    "\n",
    "\n",
    "    env = make_vec_env('EVChargingEnv-v0', n_envs=N_environments, seed=42)\n",
    "\n",
    "    env = VecNormalize(env, norm_obs=True, norm_reward=True, norm_obs_keys=['evs','power_allowed'])\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "data": {
      "text/plain": "'NOT works with vectorized environments '"
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#env = EVChargingEnv_discrete(evs=df_evs_to_load, power_limit=df_build_surplus_pwr)\n",
    "\"\"\"Function to verify environment functionality \"\"\"\n",
    "#from stable_baselines3.common.env_checker import check_env\n",
    "#check_env(env)\n",
    "\"\"\"NOT works with vectorized environments \"\"\""
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n",
      "Logging to logs/1693061730_SOC_reward_during_episode/A2C_0\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        | 24        |\n",
      "|    ep_rew_mean        | -1.29e+05 |\n",
      "| time/                 |           |\n",
      "|    fps                | 74        |\n",
      "|    iterations         | 100       |\n",
      "|    time_elapsed       | 107       |\n",
      "|    total_timesteps    | 8000      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -200      |\n",
      "|    explained_variance | 0.397     |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 99        |\n",
      "|    policy_loss        | -36.6     |\n",
      "|    value_loss         | 0.0406    |\n",
      "-------------------------------------\n",
      "Logging to logs/1693061730_SOC_reward_during_episode/A2C_0\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        | 24        |\n",
      "|    ep_rew_mean        | -1.25e+05 |\n",
      "| time/                 |           |\n",
      "|    fps                | 74        |\n",
      "|    iterations         | 100       |\n",
      "|    time_elapsed       | 108       |\n",
      "|    total_timesteps    | 18000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -195      |\n",
      "|    explained_variance | 0.0763    |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 224       |\n",
      "|    policy_loss        | -18       |\n",
      "|    value_loss         | 0.0107    |\n",
      "-------------------------------------\n",
      "Logging to logs/1693061730_SOC_reward_during_episode/A2C_0\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        | 24        |\n",
      "|    ep_rew_mean        | -1.09e+05 |\n",
      "| time/                 |           |\n",
      "|    fps                | 73        |\n",
      "|    iterations         | 100       |\n",
      "|    time_elapsed       | 108       |\n",
      "|    total_timesteps    | 28000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -181      |\n",
      "|    explained_variance | 0.865     |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 349       |\n",
      "|    policy_loss        | -1.41     |\n",
      "|    value_loss         | 0.000379  |\n",
      "-------------------------------------\n",
      "Logging to logs/1693061730_SOC_reward_during_episode/A2C_0\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        | 24        |\n",
      "|    ep_rew_mean        | -7.59e+04 |\n",
      "| time/                 |           |\n",
      "|    fps                | 67        |\n",
      "|    iterations         | 100       |\n",
      "|    time_elapsed       | 118       |\n",
      "|    total_timesteps    | 38000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -143      |\n",
      "|    explained_variance | 0.885     |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 474       |\n",
      "|    policy_loss        | -2.43     |\n",
      "|    value_loss         | 0.000698  |\n",
      "-------------------------------------\n",
      "Logging to logs/1693061730_SOC_reward_during_episode/A2C_0\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        | 24        |\n",
      "|    ep_rew_mean        | -4.92e+04 |\n",
      "| time/                 |           |\n",
      "|    fps                | 71        |\n",
      "|    iterations         | 100       |\n",
      "|    time_elapsed       | 111       |\n",
      "|    total_timesteps    | 48000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -105      |\n",
      "|    explained_variance | 0.968     |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 599       |\n",
      "|    policy_loss        | -0.667    |\n",
      "|    value_loss         | 0.000384  |\n",
      "-------------------------------------\n",
      "Logging to logs/1693061730_SOC_reward_during_episode/A2C_0\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        | 24        |\n",
      "|    ep_rew_mean        | -3.77e+04 |\n",
      "| time/                 |           |\n",
      "|    fps                | 70        |\n",
      "|    iterations         | 100       |\n",
      "|    time_elapsed       | 112       |\n",
      "|    total_timesteps    | 58000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -83.5     |\n",
      "|    explained_variance | 0.975     |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 724       |\n",
      "|    policy_loss        | 0.682     |\n",
      "|    value_loss         | 0.000895  |\n",
      "-------------------------------------\n",
      "Logging to logs/1693061730_SOC_reward_during_episode/A2C_0\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        | 24        |\n",
      "|    ep_rew_mean        | -2.86e+04 |\n",
      "| time/                 |           |\n",
      "|    fps                | 68        |\n",
      "|    iterations         | 100       |\n",
      "|    time_elapsed       | 117       |\n",
      "|    total_timesteps    | 68000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -62.8     |\n",
      "|    explained_variance | 0.965     |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 849       |\n",
      "|    policy_loss        | -0.716    |\n",
      "|    value_loss         | 0.000492  |\n",
      "-------------------------------------\n",
      "Logging to logs/1693061730_SOC_reward_during_episode/A2C_0\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        | 24        |\n",
      "|    ep_rew_mean        | -2.49e+04 |\n",
      "| time/                 |           |\n",
      "|    fps                | 70        |\n",
      "|    iterations         | 100       |\n",
      "|    time_elapsed       | 113       |\n",
      "|    total_timesteps    | 78000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -51.8     |\n",
      "|    explained_variance | 0.962     |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 974       |\n",
      "|    policy_loss        | -0.059    |\n",
      "|    value_loss         | 0.000344  |\n",
      "-------------------------------------\n",
      "Logging to logs/1693061730_SOC_reward_during_episode/A2C_0\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        | 24        |\n",
      "|    ep_rew_mean        | -2.02e+04 |\n",
      "| time/                 |           |\n",
      "|    fps                | 71        |\n",
      "|    iterations         | 100       |\n",
      "|    time_elapsed       | 111       |\n",
      "|    total_timesteps    | 88000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -41       |\n",
      "|    explained_variance | 0.965     |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1099      |\n",
      "|    policy_loss        | -0.491    |\n",
      "|    value_loss         | 0.000262  |\n",
      "-------------------------------------\n",
      "Logging to logs/1693061730_SOC_reward_during_episode/A2C_0\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        | 24        |\n",
      "|    ep_rew_mean        | -1.83e+04 |\n",
      "| time/                 |           |\n",
      "|    fps                | 70        |\n",
      "|    iterations         | 100       |\n",
      "|    time_elapsed       | 113       |\n",
      "|    total_timesteps    | 98000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -33.4     |\n",
      "|    explained_variance | -0.776    |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1224      |\n",
      "|    policy_loss        | 1.34      |\n",
      "|    value_loss         | 0.00277   |\n",
      "-------------------------------------\n",
      "Logging to logs/1693061730_SOC_reward_during_episode/A2C_0\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        | 24        |\n",
      "|    ep_rew_mean        | -1.31e+04 |\n",
      "| time/                 |           |\n",
      "|    fps                | 69        |\n",
      "|    iterations         | 100       |\n",
      "|    time_elapsed       | 115       |\n",
      "|    total_timesteps    | 108000    |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -39.4     |\n",
      "|    explained_variance | 0.933     |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1349      |\n",
      "|    policy_loss        | 1.83      |\n",
      "|    value_loss         | 0.00624   |\n",
      "-------------------------------------\n",
      "Logging to logs/1693061730_SOC_reward_during_episode/A2C_0\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        | 24        |\n",
      "|    ep_rew_mean        | -1.15e+04 |\n",
      "| time/                 |           |\n",
      "|    fps                | 65        |\n",
      "|    iterations         | 100       |\n",
      "|    time_elapsed       | 121       |\n",
      "|    total_timesteps    | 118000    |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -46.6     |\n",
      "|    explained_variance | 0.472     |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1474      |\n",
      "|    policy_loss        | -6.02     |\n",
      "|    value_loss         | 0.0288    |\n",
      "-------------------------------------\n",
      "Logging to logs/1693061730_SOC_reward_during_episode/A2C_0\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 24       |\n",
      "|    ep_rew_mean        | -9e+03   |\n",
      "| time/                 |          |\n",
      "|    fps                | 71       |\n",
      "|    iterations         | 100      |\n",
      "|    time_elapsed       | 112      |\n",
      "|    total_timesteps    | 128000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -55.9    |\n",
      "|    explained_variance | 0.862    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 1599     |\n",
      "|    policy_loss        | -1.85    |\n",
      "|    value_loss         | 0.0041   |\n",
      "------------------------------------\n",
      "Logging to logs/1693061730_SOC_reward_during_episode/A2C_0\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 24       |\n",
      "|    ep_rew_mean        | -8e+03   |\n",
      "| time/                 |          |\n",
      "|    fps                | 71       |\n",
      "|    iterations         | 100      |\n",
      "|    time_elapsed       | 111      |\n",
      "|    total_timesteps    | 138000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -53.6    |\n",
      "|    explained_variance | 0.727    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 1724     |\n",
      "|    policy_loss        | -4.93    |\n",
      "|    value_loss         | 0.0088   |\n",
      "------------------------------------\n",
      "Logging to logs/1693061730_SOC_reward_during_episode/A2C_0\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        | 24        |\n",
      "|    ep_rew_mean        | -7.31e+03 |\n",
      "| time/                 |           |\n",
      "|    fps                | 70        |\n",
      "|    iterations         | 100       |\n",
      "|    time_elapsed       | 113       |\n",
      "|    total_timesteps    | 148000    |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -47.7     |\n",
      "|    explained_variance | 0.968     |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1849      |\n",
      "|    policy_loss        | 0.73      |\n",
      "|    value_loss         | 0.000486  |\n",
      "-------------------------------------\n",
      "Logging to logs/1693061730_SOC_reward_during_episode/A2C_0\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        | 24        |\n",
      "|    ep_rew_mean        | -6.77e+03 |\n",
      "| time/                 |           |\n",
      "|    fps                | 69        |\n",
      "|    iterations         | 100       |\n",
      "|    time_elapsed       | 115       |\n",
      "|    total_timesteps    | 158000    |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -32.9     |\n",
      "|    explained_variance | 0.974     |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1974      |\n",
      "|    policy_loss        | -0.855    |\n",
      "|    value_loss         | 0.000559  |\n",
      "-------------------------------------\n",
      "Eval num_timesteps=160000, episode_reward=-5484.40 +/- 0.00\n",
      "Episode length: 24.00 +/- 0.00\n",
      "-------------------------------------\n",
      "| eval/                 |           |\n",
      "|    mean_ep_length     | 24        |\n",
      "|    mean_reward        | -5.48e+03 |\n",
      "| time/                 |           |\n",
      "|    total_timesteps    | 160000    |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -8.56     |\n",
      "|    explained_variance | 0.494     |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1999      |\n",
      "|    policy_loss        | -0.0172   |\n",
      "|    value_loss         | 1.75e-05  |\n",
      "-------------------------------------\n",
      "New best mean reward!\n",
      "Logging to logs/1693061730_SOC_reward_during_episode/A2C_0\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        | 24        |\n",
      "|    ep_rew_mean        | -6.42e+03 |\n",
      "| time/                 |           |\n",
      "|    fps                | 72        |\n",
      "|    iterations         | 100       |\n",
      "|    time_elapsed       | 110       |\n",
      "|    total_timesteps    | 168000    |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -7.66     |\n",
      "|    explained_variance | 0.745     |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 2099      |\n",
      "|    policy_loss        | -0.0194   |\n",
      "|    value_loss         | 2.51e-05  |\n",
      "-------------------------------------\n",
      "Logging to logs/1693061730_SOC_reward_during_episode/A2C_0\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        | 24        |\n",
      "|    ep_rew_mean        | -6.06e+03 |\n",
      "| time/                 |           |\n",
      "|    fps                | 70        |\n",
      "|    iterations         | 100       |\n",
      "|    time_elapsed       | 114       |\n",
      "|    total_timesteps    | 178000    |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -6.32     |\n",
      "|    explained_variance | -0.0518   |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 2224      |\n",
      "|    policy_loss        | 0.00408   |\n",
      "|    value_loss         | 2.07e-05  |\n",
      "-------------------------------------\n",
      "Logging to logs/1693061730_SOC_reward_during_episode/A2C_0\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        | 24        |\n",
      "|    ep_rew_mean        | -5.84e+03 |\n",
      "| time/                 |           |\n",
      "|    fps                | 66        |\n",
      "|    iterations         | 100       |\n",
      "|    time_elapsed       | 120       |\n",
      "|    total_timesteps    | 188000    |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -5.1      |\n",
      "|    explained_variance | 0.73      |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 2349      |\n",
      "|    policy_loss        | 0.0129    |\n",
      "|    value_loss         | 2.61e-05  |\n",
      "-------------------------------------\n",
      "Logging to logs/1693061730_SOC_reward_during_episode/A2C_0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\"\"\" RUN algorithms \"\"\"\n",
    "os.chdir('C:/Users/dtubiana/PycharmProjects/chargeSystem/eval_model_V6')\n",
    "models_dir = f\"models/{int(time.time())}_{run_name}/\"\n",
    "logdir = f\"logs/{int(time.time())}_{run_name}/\"\n",
    "\n",
    "from stable_baselines3.common.callbacks import EvalCallback\n",
    "eval_callback = EvalCallback(env, log_path='./logs/')\n",
    "\n",
    "if not os.path.exists(models_dir):\n",
    "\tos.makedirs(models_dir)\n",
    "\n",
    "if not os.path.exists(logdir):\n",
    "\tos.makedirs(logdir)\n",
    "\n",
    "env.reset()\n",
    "\n",
    "#model = PPO('MultiInputPolicy', env, verbose=2, tensorboard_log=logdir, n_steps=512)\n",
    "model = A2C('MultiInputPolicy', env, verbose=2, tensorboard_log=logdir)\n",
    "# save model every N steps\n",
    "TIMESTEPS = 10000\n",
    "iters = 0\n",
    "while True:\n",
    "\titers += 1\n",
    "\tmodel.learn(total_timesteps=TIMESTEPS, reset_num_timesteps=False, tb_log_name=f\"A2C\", callback=eval_callback) # TODO: model name\n",
    "\tmodel.save(f\"{models_dir}/{run_name}{TIMESTEPS*iters}\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "tensorboard --logdir C:/Users/dtubiana/PycharmProjects/chargeSystem/eval_model_V6/logs\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [],
   "source": [
    "\"\"\"Load model\"\"\"\n",
    "model = A2C.load('C:/Users/dtubiana/PycharmProjects/chargeSystem/eval_model_V6/models/1693055432_power_reward_unclip/power_reward_unclip240000.zip')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\dtubiana\\PycharmProjects\\chargeSystem\\venv\\lib\\site-packages\\gymnasium\\envs\\registration.py:693: UserWarning: \u001B[33mWARN: Overriding environment EVChargingEnv-v0 already in registry.\u001B[0m\n",
      "  logger.warn(f\"Overriding environment {new_spec.id} already in registry.\")\n",
      "C:\\Users\\dtubiana\\PycharmProjects\\chargeSystem\\venv\\lib\\site-packages\\gymnasium\\utils\\passive_env_checker.py:42: UserWarning: \u001B[33mWARN: A Box observation space has an unconventional shape (neither an image, nor a 1D vector). We recommend flattening the observation to have only a 1D vector or use a custom policy to properly process the data. Actual observation shape: (189, 5)\u001B[0m\n",
      "  logger.warn(\n"
     ]
    }
   ],
   "source": [
    "\"\"\"Create new environment for test data - Run over the last enviroment\"\"\"\n",
    "\n",
    "df_evs_to_load = df_evs_test.to_dict('records')\n",
    "df_build_surplus_pwr = df_build['surplus_power[kw]'].to_numpy()\n",
    "\n",
    "N_environments = 16\n",
    "if ENV_type == 0:\n",
    "    \"Continues environment\"\n",
    "    from gymnasium.envs.registration import register\n",
    "\n",
    "    register(\n",
    "        id= 'EVChargingEnv-v0',\n",
    "        entry_point='EVChargingEnv_continues_reward:make_ev_charging_env',\n",
    "        kwargs={'evs': df_evs_to_load, 'power_limit': df_build_surplus_pwr})\n",
    "\n",
    "\n",
    "    env = make_vec_env('EVChargingEnv-v0', n_envs=N_environments, seed=42)\n",
    "\n",
    "    env = VecNormalize(env, norm_obs=True, norm_reward=True, norm_obs_keys=['evs','power_allowed'])\n",
    "\n",
    "elif ENV_type == 1:\n",
    "    \"Discrete environment\"\n",
    "    from gymnasium.envs.registration import register\n",
    "\n",
    "    register(\n",
    "        id= 'EVChargingEnv-v0',\n",
    "        entry_point='EVChargingEnv_discrete_reward:make_ev_charging_env',\n",
    "        kwargs={'evs': df_evs_to_load, 'power_limit': df_build_surplus_pwr})\n",
    "\n",
    "\n",
    "    env = make_vec_env('EVChargingEnv-v0', n_envs=N_environments, seed=42)\n",
    "\n",
    "    env = VecNormalize(env, norm_obs=True, norm_reward=True, norm_obs_keys=['evs','power_allowed'])\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 1, chargePWR: 1252.8000000000002, Power allowed 481.7926330566406\n",
      "\n",
      "Time: 2, chargePWR: 55.0, Power allowed 490.21636962890625\n",
      "\n",
      "Time: 3, chargePWR: 36.7, Power allowed 494.0168762207031\n",
      "\n",
      "Time: 4, chargePWR: 29.4, Power allowed 495.3977355957031\n",
      "\n",
      "Time: 5, chargePWR: 22.1, Power allowed 496.5660705566406\n",
      "\n",
      "Time: 6, chargePWR: 44.0, Power allowed 497.13995361328125\n",
      "\n",
      "Time: 7, chargePWR: 29.4, Power allowed 470.9508972167969\n",
      "\n",
      "Time: 8, chargePWR: 25.7, Power allowed 435.3732604980469\n",
      "\n",
      "Time: 9, chargePWR: 47.7, Power allowed 468.2096252441406\n",
      "\n",
      "Time: 10, chargePWR: 29.4, Power allowed 485.6842041015625\n",
      "\n",
      "Time: 11, chargePWR: 36.7, Power allowed 487.84112548828125\n",
      "\n",
      "Time: 12, chargePWR: 29.4, Power allowed 487.2452697753906\n",
      "\n",
      "Time: 13, chargePWR: 33.0, Power allowed 484.8127746582031\n",
      "\n",
      "Time: 14, chargePWR: 25.7, Power allowed 487.64337158203125\n",
      "\n",
      "Time: 15, chargePWR: 29.4, Power allowed 489.7359924316406\n",
      "\n",
      "Time: 16, chargePWR: 25.7, Power allowed 488.6941833496094\n",
      "\n",
      "Time: 17, chargePWR: 29.4, Power allowed 482.49139404296875\n",
      "\n",
      "Time: 18, chargePWR: 55.0, Power allowed 466.40740966796875\n",
      "\n",
      "Time: 19, chargePWR: 33.0, Power allowed 437.3492431640625\n",
      "\n",
      "Time: 20, chargePWR: 25.7, Power allowed 403.87017822265625\n",
      "\n",
      "Time: 21, chargePWR: 33.0, Power allowed 395.3785705566406\n",
      "\n",
      "Time: 22, chargePWR: 36.7, Power allowed 391.29302978515625\n",
      "\n",
      "Time: 23, chargePWR: 29.4, Power allowed 426.28973388671875\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\"\"\"Run algo for predictions\"\"\"\n",
    "N_steps = 24\n",
    "charge_pwr = np.zeros( (N_steps,N_environments) )\n",
    "\n",
    "# TODO: run prediction on test set to prevent over fitting\n",
    "obs = env.reset()\n",
    "obs = env.get_original_obs()\n",
    "\n",
    "# Run 23 hours\n",
    "for i in range(23):\n",
    "    action, _states = model.predict(obs)\n",
    "\n",
    "    obs, rewards, done, info = env.step(action)\n",
    "    for env_num in range(N_environments):\n",
    "        charge_pwr[i,env_num] = (info[env_num]['charge_pwr'])\n",
    "    p = info[10]['charge_pwr']\n",
    "    unnormalized_obs = env.get_original_obs()\n",
    "    t = unnormalized_obs['time'][1]\n",
    "    pa = unnormalized_obs['power_allowed'][0][t-1]\n",
    "    print(f'Time: {t}, chargePWR: {p}, Power allowed {pa}')\n",
    "    print()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [],
   "source": [
    "SOC_mat = np.zeros((len(df_evs_to_load), N_environments))\n",
    "for env_num in range(N_environments):\n",
    "        SOC_mat[:,env_num] = (((unnormalized_obs['evs'][env_num])[:,4]).tolist())\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total_reward mean :total_reward   -6491.264188\n",
      "dtype: float64 ,total_reward std:  total_reward    975.592709\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "total_reward =[]\n",
    "for env_num in range(N_environments):\n",
    "    total_reward.append(info[env_num]['total_reward'])\n",
    "\n",
    "total_reward = np.array(total_reward)\n",
    "total_reward = pd.DataFrame(total_reward, columns=['total_reward'])\n",
    "tr_m = total_reward.mean()\n",
    "tr_std = total_reward.std()\n",
    "print(f'total_reward mean :{tr_m} ,total_reward std:  {tr_std}')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [
    {
     "data": {
      "text/plain": "     initial_SOC  Arrival_time[h]  TuD (int)  Battery capacity [KWh]  \\\n0       0.456038             10.0        0.0                    57.5   \n1       0.375950             22.0        0.0                    77.0   \n2       0.449382             15.0        0.0                    70.0   \n3       0.323799             15.0        0.0                    64.0   \n4       0.416972             18.0        0.0                    70.0   \n..           ...              ...        ...                     ...   \n184     0.351860              6.0        0.0                    57.5   \n185     0.414090              5.0        0.0                    70.0   \n186     0.175688              1.0        0.0                    70.0   \n187     0.206499              1.0        0.0                    57.5   \n188     0.304227              1.0        0.0                    57.5   \n\n        ENonD       SOC  \n0    5.401042  0.520386  \n1    8.337047  0.518807  \n2    9.631797  0.449382  \n3    7.331303  0.495674  \n4    8.720966  0.469829  \n..        ...       ...  \n184  7.976377  0.416208  \n185  9.813478  0.571232  \n186  8.518055  0.228545  \n187  7.949520  0.270847  \n188  4.814002  0.368575  \n\n[189 rows x 6 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>initial_SOC</th>\n      <th>Arrival_time[h]</th>\n      <th>TuD (int)</th>\n      <th>Battery capacity [KWh]</th>\n      <th>ENonD</th>\n      <th>SOC</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0.456038</td>\n      <td>10.0</td>\n      <td>0.0</td>\n      <td>57.5</td>\n      <td>5.401042</td>\n      <td>0.520386</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0.375950</td>\n      <td>22.0</td>\n      <td>0.0</td>\n      <td>77.0</td>\n      <td>8.337047</td>\n      <td>0.518807</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0.449382</td>\n      <td>15.0</td>\n      <td>0.0</td>\n      <td>70.0</td>\n      <td>9.631797</td>\n      <td>0.449382</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0.323799</td>\n      <td>15.0</td>\n      <td>0.0</td>\n      <td>64.0</td>\n      <td>7.331303</td>\n      <td>0.495674</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0.416972</td>\n      <td>18.0</td>\n      <td>0.0</td>\n      <td>70.0</td>\n      <td>8.720966</td>\n      <td>0.469829</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>184</th>\n      <td>0.351860</td>\n      <td>6.0</td>\n      <td>0.0</td>\n      <td>57.5</td>\n      <td>7.976377</td>\n      <td>0.416208</td>\n    </tr>\n    <tr>\n      <th>185</th>\n      <td>0.414090</td>\n      <td>5.0</td>\n      <td>0.0</td>\n      <td>70.0</td>\n      <td>9.813478</td>\n      <td>0.571232</td>\n    </tr>\n    <tr>\n      <th>186</th>\n      <td>0.175688</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>70.0</td>\n      <td>8.518055</td>\n      <td>0.228545</td>\n    </tr>\n    <tr>\n      <th>187</th>\n      <td>0.206499</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>57.5</td>\n      <td>7.949520</td>\n      <td>0.270847</td>\n    </tr>\n    <tr>\n      <th>188</th>\n      <td>0.304227</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>57.5</td>\n      <td>4.814002</td>\n      <td>0.368575</td>\n    </tr>\n  </tbody>\n</table>\n<p>189 rows × 6 columns</p>\n</div>"
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "df_evs_out = pd.concat([df_evs_out, pd.DataFrame(unnormalized_obs['evs'][0], columns= ['Arrival_time[h]', 'TuD (int)', 'Battery capacity [KWh]', 'ENonD' ,'SOC'])], axis=1)\n",
    "#df_evs_out['initial_SOC']  = pd.DataFrame(df_evs_test)['SOC'] # get initial soc to calc soc diffrance\n",
    "df_evs_out"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [
    {
     "data": {
      "text/plain": "48"
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# concatenate surplus power to charge power for comparison\n",
    "charge_pwr_df = pd.DataFrame(charge_pwr)\n",
    "charge_pwr_df = pd.concat([charge_pwr_df,pd.Series(df_build_surplus_pwr, name= 'surplusPWR')], axis=1)\n",
    "((df_evs_out['SOC'] - (df_evs_out['ENonD']/df_evs_out['Battery capacity [KWh]'] + 0.2)) < 0).sum()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [],
   "source": [
    "\"\"\"EXPORT data to excel files\"\"\"\n",
    "\n",
    "os.chdir('C:/Users/dtubiana/PycharmProjects/chargeSystem/eval_model_V6')\n",
    "\n",
    "run_results_dir = f\"run_results/{int(time.time())}_{run_name}/\"\n",
    "\n",
    "if not os.path.exists(run_results_dir):\n",
    "\tos.makedirs(run_results_dir)\n",
    "\n",
    "os.chdir('C:/Users/dtubiana/PycharmProjects/chargeSystem/eval_model_V6/'+run_results_dir)\n",
    "file_name = run_name + '.xlsx'\n",
    "writer = pd.ExcelWriter(file_name, engine='xlsxwriter')\n",
    "\n",
    "\n",
    "charge_pwr_df.to_excel(writer, sheet_name='charge_pwr', index=False )\n",
    "SOC_mat_df = pd.DataFrame(SOC_mat)\n",
    "SOC_mat_df.to_excel(writer, sheet_name='SOC_mat', index=False)\n",
    "\n",
    "df_evs_out.to_excel(writer, sheet_name='df_evs_out', index=False)\n",
    "total_reward.to_excel(writer, sheet_name='total_reward', index=False)\n",
    "# Save and close the writer\n",
    "writer.close()"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ]
}
